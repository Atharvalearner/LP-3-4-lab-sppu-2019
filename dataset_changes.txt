# FEEDFORWARD NEURAL 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import SGD
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder

# 1. Load the dataset (replace 'your_dataset.csv' with your file path)
data = pd.read_csv('your_dataset.csv')

# 2. Separate features (X) and labels (y)
X = data.iloc[:, :-1].values  # All columns except the last one
y = data.iloc[:, -1].values   # Last column

# 3. Normalize the features (assuming pixel values range from 0 to 255)
X = X / 255.0  # Scales pixel values to [0, 1]

# 4. Reshape features if working with image data
# For example, if each sample represents a 28x28 image
X = X.reshape(-1, 28, 28)  # Adjust dimensions based on image size

# 5. One-hot encode the labels if using categorical cross-entropy
encoder = OneHotEncoder(sparse=False)
y = encoder.fit_transform(y.reshape(-1, 1))

# 6. Split data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 7. Define the neural network model
model = Sequential([
    Flatten(input_shape=(28, 28)),     # Adjust input shape if necessary
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# 8. Compile the model
model.compile(optimizer=SGD(learning_rate=0.01), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

# 9. Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test))

# 10. Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f"Test loss: {test_loss:.4f}, Test accuracy: {test_accuracy:.4f}")

# 11. Plot training and validation accuracy
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()




# IMAGE CLASSIFICATION 
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define the paths to the dataset folders
train_dir = 'path/to/train'  # Path to the training images directory
test_dir = 'path/to/test'    # Path to the testing images directory

# Stage a: Loading and preprocessing the image data
# Use ImageDataGenerator to load and preprocess the data
train_datagen = ImageDataGenerator(rescale=1.0/255.0)  # Normalize the pixel values
test_datagen = ImageDataGenerator(rescale=1.0/255.0)

# Load the training and testing data from directories
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(32, 32),    # Resize all images to 32x32 pixels
    batch_size=64,
    class_mode='sparse'      # For integer labels
)

test_generator = test_datagen.flow_from_directory(
    test_dir,
    target_size=(32, 32),
    batch_size=64,
    class_mode='sparse'
)

# Stage b: Defining the model’s architecture
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(train_generator.num_classes, activation='softmax')  # Adjust output units to match number of classes
])

# Stage c: Compiling and training the model
model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])

model.fit(train_generator, epochs=10, validation_data=test_generator)

# Stage d: Estimating the model’s performance
test_loss, test_accuracy = model.evaluate(test_generator)
print(f'Test Accuracy: {test_accuracy * 100:.2f}%')
